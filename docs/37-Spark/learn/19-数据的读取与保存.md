# 1 数据的读取与保存

Spark支持多种数据输入输出源。一部分原因是因为Spark本身是基本Hadoop生态圈构建的，特别是Spark可以通过 MapReduce 所使用的 InputFormat 和 OutPutFormat 接口访问数据，大部分的文件格式与存储系统（HDFS、HBase、Cassandra等）都支持使用  InputFormat 和 OutPutFormat 接口。



# 2 文件格式与文件系统

## 1 文件格式

Spark的数据读取及数据保存可以从两个维度来作区分：文件格式以及文件系统。

-   文件格式分为：text文件、csv文件、sequence文件以及Object文件；如果需要读取其他格式的数据，可以使用SparkSession对象来读取
-   文件系统分为：本地文件系统、HDFS、HBASE以及数据库。

Spark支持的一些常见格式

| 格式名称         | 结构化   | 备注                                                         |
| ---------------- | -------- | ------------------------------------------------------------ |
| 文本文件         | 否       | 普通的文本文件，每行记录一条记录                             |
| JSON             | 半结构化 |                                                              |
| CSV              | 是       | 常见的基于文本的格式，通常在电子表格中应用                   |
| SequenceFiles    | 是       | 一种用与键值对数据的常见Hadoop格式                           |
| Protocol buffers | 是       | 一种快速、节约空间的跨语言格式                               |
| 对象文件         | 是       | 用来将Spark作业中的数据存储下来以让共享的代码读取。<br>改变类的时候会失效，因为它依赖于Java序列化 |



### 2.1 文本文件（text）

Spark读取文本文件非常容易。当我们将文本文件读取为RDD时，输入的每一行都会成为RDD中的一个元素。

我们也可以将多个完整的文本文件一次性读取为 Pair RDD，其中键是文件名，值是文件内容。

**1）读取文本文件**

```scala
// 以行为单位来读取数据，读取的数据都是字符串
def textFile(path: String): JavaRDD[String]
// minPartitions设置最小分区数量 
// 注意：分区数量可能大于 minPartitions
def textFile(path: String, minPartitions: Int): JavaRDD[String]

// spark读取文件时，底层其实使用的就是Hadoop的读取方式，所以是一行一行读取
// 数据读取是以偏移量为单位，偏移量不会被重复读取
    // totalSize
    // goalSize = totalSize/minPartitions


// 读取指定路径下的多个文本文件，并将文件内容以键值对的形式返回一个 JavaPairRDD 对象
// 以文件为单位读取数据，读取的结果是一个元组
def wholeTextFiles(path: String): JavaPairRDD[String, String]
// 第一个元素表示文件路径(可以是一个具体的文件路径，也可以是一个目录路径)，第二个元素表示文件内容
def wholeTextFiles(path: String, minPartitions: Int): JavaPairRDD[String, String]
```

例如：

在 Python 中读取一个文本文件

```python
input = sc.textFile("file:///home/nhk/repos/spark/README.md");
```

在 Scala 中读取一个文本文件

```scala
val input = sc.textFile("file:///home/nhk/repos/spark/README.md"); 
```

在 Java 中读取一个文本文件

```java
JavaRDD<String> input = jsc.textFile("file:///home/nhk/repos/spark/README.md");
```

提示：

​	**Spark支持读取给定目录中所有的文件。**

​	**Spark还支持在输入路径中使用通配符。例如 jsc.textFile("part-*.txt");**

**2）保存文本文件**

Spark会将传入的路径path作为目录对待，并将RDD中的内容输出到这个目录中（一般会输入多个文件，这样Spark就可以从多个节点并行输出。）

但是我们并不能控制那一部分的内容输出到哪一个文件中，不过我们可以控制输出的格式

```java
// 保存成Text文件
def saveAsTextFile(path: String): Unit
def saveAsTextFile(path: String, codec: Class[_ <: CompressionCodec]): Unit
// 将数据集的元素作为文本文件(或文本文件集)写入本地文件系统、HDFS或任何其他hadoop支持的文件系统的给定目录中。
// Spark将对RDD中的每个元素调用toString，将其转换为文件中的一行文本。    
    
    
// 序列化成对象保存到文件
def saveAsTextFile(path: String, codec: Class[_ <: CompressionCodec]): Unit
// 保存成SequenceFile文件  此方法要求数据格式必须为 K-V 类型
```

方法说明：

-   saveAsTextFile用于将RDD以文本文件的格式存储到文件系统中。

    -   从源码中可以看到，**saveAsTextFile函数是依赖于saveAsHadoopFile函数**，由于saveAsHadoopFile函数接受PairRDD，所以在saveAsTextFile函数中利用rddToPairRDDFunctions函数转化为(NullWritable,Text)类型的RDD，然后通过saveAsHadoopFile函数实现相应的写操作。

    例如：

    ```scala
    def saveAsTextFile(path: String, codec: Class[_ <: CompressionCodec]): Unit = withScope {
        this.mapPartitions { iter =>
            val text = new Text()
            iter.map { x =>
                require(x != null, "text files do not allow null rows")
                text.set(x.toString)
                (NullWritable.get(), text)
            }
        }.saveAsHadoopFile[TextOutputFormat[NullWritable, Text]](path, codec)
    }
    ```

    

-   saveAsObjectFile用于将RDD中的元素序列化成对象，存储到文件中。

-   从源码中可以看出，saveAsObjectFile函数是依赖于saveAsSequenceFile函数实现的，如下：

```scala
def saveAsObjectFile(path: String): Unit = {
    rdd.saveAsObjectFile(path)
}
```

演示：

```java
public static void main(String[] args) {
    SparkConf sparkConf = new SparkConf().setMaster("local[*]").setAppName("text文件");
    JavaSparkContext sparkContext = new JavaSparkContext(sparkConf);
    // 读取输入文件
    JavaRDD<String> javaRDD = sparkContext.textFile("file:///opt/temp/hello.txt");
    // 保存数据
    javaRDD.saveAsTextFile("file:///opt/output");
}
```

### 2.2 JSON

​	Spark Core中并没有提供直接读取JSON数据的算子，但是我们可用通过读取文本文件（如：textFile()）的数据，然后利用JSON解析库解析成JSON。(换句话说，就是**通过读取文本文件并加以解析以读取JSON数据**)

在Java 和 Scala中，我们可以使用自定义的Hadoop格式来读取JSON数据。

**1）读取JSON**

将数据作为文本读取，然后对JSON数据进行解析。这种的方法绝大部分编程语言都可以使用。

这种方法假设我们只能读取文件中每一行数据都是JSON记录的。如果JSON数据是跨行的，那么只能读取整个文件，然后再对文件进行解析了。构建JSON解析器开销较大，我们可以使用 mapPartition()算子重用解析器进行优化。

Python、Java、Scala中都有大量可用的JSON库。我们只演示Python的内建的库，和Java、Scala中的Jackson库。

演示：

在 Python 在读取半结构化数据JSON

```python
import json

data = input.map(lambda x: json.loads(x))
```

在 Scala 、Java 在读取半结构化数据JSON

说明：

​	在Scala、Java 中通常使用一个类来接收JSON中我们想要的数据。

Scala：

```scala
case class Person(name: String, lovesPandas: Boolean) // 必须为顶级类


// 将其解析为特定的case class。
val result = input.flatMap(record => {
    try {
        Some(mapper.readValue(record, classOf[Person]))
    } catch {
        case e:Exception => None // 遇到问题返回None
    }
})
```

Java：

```java
import lombok.Data;
import lombok.NoArgsConstructor;
import lombok.NonNull;

@Data
@NoArgsConstructor
public class Person {  // 该类必须为顶级类
    @NonNull
    private String name;
    @NonNull
    private boolean lovesPandas;
}


public class SparkRDD_json {
    public static void main(String[] args) {
        SparkConf sparkConf = new SparkConf().setAppName("读取JSON数据");
        JavaSparkContext jsc = new JavaSparkContext(sparkConf);
        JavaRDD<String> input = jsc.textFile("file.json");
        JavaRDD<Person> result = input.mapPartitions(new ParseJson());
    }
}

class ParseJson implements FlatMapFunction<Iterator<String>, Person> {
    @Override
    public Iterator<Person> call(Iterator<String> lines) throws Exception {
        ArrayList<Person> people = new ArrayList<>();
        ObjectMapper mapper = new ObjectMapper();
        while (lines.hasNext()) {
            String line = lines.next();
            try {
                people.add(mapper.readValue(line, Person.class));
            } catch (JsonProcessingException e) {
                // 跳过失败的数据
            }
        }
        return people.iterator();
    }
}
```

注意：

​	处理格式不正确的记录可能会引发严重的问题。对于JSON这种半结构化数据，小规模的数据可以不处理



**2）保存JSON**

我们可以将字符串 RDD 通过JSON解析库转换为JSON数据，再将结构化的数据组成的RDD 转换为字符串 RDD，然后再利用Spark提供的文本文件 API写出去（如：saveAsTextFile）。 

演示：

在 Python 中保存为JSON

```python
(data.filter(lambda x: x["lovesPandas"]).map(lambda x: josn.dumps(x))
	.saveAsTextFile(outputFile))	# Spark中并没有将RDD直接保存为JSON的算子，所以需要先将RDD解析为JSON数据，再通过文本文件形式将数据保存
```

在 Scala 在保存为 JSON

```scala
result.filter(p => p.lovesPandas).map(mapper.writerValueAsString(_))
	.saveAsTextFile(outputFile)
```

在 Java 中保存为 JSON

```java
class WriterJson implements FlatMapFunction<Iterator<Person>, String> {
    @Override
    public Iterator<String> call(Iterator<Person> people) throws Exception {
        ArrayList<String> text = new ArrayList<>();
        ObjectMapper mapper = new ObjectMapper();
        while (people.hasNext()) {
            Person person = people.next();
            text.add(mapper.writeValueAsString(person));
        }
        return text.iterator();
    }
}


...
JavaRDD<Person> result = input.mapPartitions(new ParseJson()).filter(new LikesPands());

JavaRDD<String> formatted = result.mapPartitions(new WriterJson());
formatted.saveAsTextFile(outFile);
```

### 2.3 CSV 及特殊分隔值

读取CSV文件与读取JSON文件类型，这里不做演示

Spark也没有提供直接读取CSV、保存CSV的算子，都要借助于读取文本文件的算子进行操作（就是**通过读取文本文件并加以解析以读取CSV数据**)）

### **2.4 SequenceFile文件**

SequenceFile文件是hadoop用来存储二进制形式的key-value对而设计的一种平面文件(Flat File)。

SequenceFile 也是 MapReduce作业中常用的输入输出格式。

由于Hadoop 使用了一套自定义的序列化框架，因此SequenceFile是由实现Hadoop的 Writable 接口的元素组成。

**下面是常见的Hadoop封装的数据类型**

Hadoop提供了如下内容的数据类型，**这些数据类型都实现了WritableComparable接口**，以便用这些类型定义的数据可以被序列化进行网络传输和文件存储，以及进行大小比较。

| Hadoop数据类型   | Java数据类型 |
| ---------------- | ------------ |
| BooleanWritable  | boolean      |
| ByteWritable     | byte         |
| IntWritable      | int          |
| FloatWritable    | float        |
| LongWritable     | long         |
| DoubleWritable   | double       |
| **Text**         | String       |
| MapWritable      | map          |
| ArrayWritable    | array        |
| **NullWritable** | null         |

如果Hadoop提供的这些数据类型无法满足我们的需求，我们可以自定义Writable类，实现WritableComparable接口。

#### **1）读取SequenceFile**

Spark中有专门的SequenceFile接口。在SparkContext中，可以调用如下方法

```scala
def sequenceFile[K, V](path: String,
    keyClass: Class[K],
    valueClass: Class[V],
    minPartitions: Int
    ): JavaPairRDD[K, V]    
    
def sequenceFile[K, V](path: String, keyClass: Class[K], valueClass: Class[V]):
  JavaPairRDD[K, V]

// 说明
//  keyClass valueClass 都必须使用正确的Writable类
```

演示：

在 Python 中读取SequenceFile

```python
val data = sc.SequenceFile(inFiles,
               "org.apache.hadoop.io.Text","org.apache.hadoop.io.IntWritable");
```

在 Scala 中读取SequenceFile

```scala
val data = sc.sequenceFile(inFile, classOf[Text],classOf[IntWritable]
                           .map{case (x,y) => (x.toString, y.get())})
```

说明：

​	在Scala在有一个很方便的函数，可以自动将Writeable对象转换为相应的Scala类型。可以调用sequenceFile 返回Scala原生数据类型的RDD，而无需指定KeyClass、ValueClass。

在 Java 中读取 SequenceFile

```java
public class ConvertToNativeTypes implements
    PairFunction<Tuple2<Text, IntWritable>, String, Integer> {
    @Override
    public Tuple2<String, Integer> call(Tuple2<Text, IntWritable> record) throws Exception {
        return new Tuple2<>(record._1.toString(), record._2.get());
    }
}


public class SparkRDD_SequenceFile {
    public static void main(String[] args) {
        SparkConf sparkConf = new SparkConf().setMaster("local[*]").setAppName("sequence文件");
        JavaSparkContext jsc = new JavaSparkContext(sparkConf);
        String fileName = "省略";
        // 读取输入文件（sequenceFile）
        JavaPairRDD<Text, IntWritable> input = jsc.sequenceFile(fileName, Text.class, IntWritable.class);
        JavaPairRDD result = input.mapToPair(new ConvertToNativeTypes());
    }
}
```

#### **2）保存sequenceFile**

Scala中的隐式转换会为我们将 Scala的原生类型转换为 Hadoop Writable 类型。所以我们直接书写Scala原生类型即可。

```scala
// saveAsSequenceFile  保存成SequenceFile文件  此方法要求数据格式必须为 K-V 类型


// 注意：在Java中保存 SequenceFile 文件比较复杂。
//		因为JavaPairRDD上没有提供saveAsSequenceFile算子。所以我们要使用Spark保存自定义Hadoop格式的功能来实现。 
```

演示：

在 Scala中保存SequenceFile

```scala
val data = sc.parallelize(List("Panda", 3), ("Kay", 6), ("Snail", 2))
data.saveAsSequenceFile(outputFile)
```

特点强调：

​	在Java中保存 SequenceFile 文件比较复杂，因为**JavaPairRDD上没有提供saveAsSequenceFile算子**。所以我们要使用Spark保存自定义Hadoop格式的功能来实现。

#### Hadoop 输入输出格式

除了Spark封装的格式之外，也可以在任何Hadoop支持的格式交互。Hadoop支持新旧两套Hadoop文件API（个人更倾向于新的那套API）。

##### **1）读取其他Hadoop输入格式**

要想使用新版的 Hadoop API 读取文件，需要告诉Spark一些东西。

newAPIHadoopFile 主要接收一个路径path以及三个类。

```scala
def newAPIHadoopFile[K, V, F <: NewInputFormat[K, V]](
    path: String,
    fClass: Class[F],	// “格式”类，代表输入格式
    kClass: Class[K],	// 键的类
    vClass: Class[V],	// 值的类
    conf: Configuration): JavaPairRDD[K, V]	// conf参数用于设定额外的Hadoop配置属性。

def newAPIHadoopRDD[K, V, F <: NewInputFormat[K, V]](
    conf: Configuration,
    fClass: Class[F],
    kClass: Class[K],
    vClass: Class[V]): JavaPairRDD[K, V]
```

其中：

​	fClass 参数可以传入一个 KeyValueTextInputFormat（这是最简单的Hadoop输入格式之一），可以用于从文本文件中读取键值对数据。每一行都会被单独处理，键和值之间用制表符隔开。

使用旧的Hadoop API 读取文件用法上几乎一样，除了需要提供旧式的 InputFormat类。Spark许多自带的封装好的函数（比如sequenceFile()）都是使用旧式Hadoop API实现的。

例如：

在 Scala 中使用老式API读取  KeyValueTextInputFormat()。

```scala
val input = sc.hadoopFile[Text, Text, KeyValueTextInputFormat](inputFile)
	.map{
        case (x, y) => (x.toString, y.toString)
    }
```

 **说明：**

​	我们也可以通过自定义Hadoop输入格式的方式读取JSON数据。Twitter的Elephant Bird 包（https://github.com/twitter/elephant-bird） 支持很多种数据格式，包括JSON、Lucene、Protocol Buffer相关的格式等。这个包适合于新旧两种 Hadoop 文件 API。

演示：

在Scala 中使用 Elephant Bird 读取 LZO算法压缩的JSON文件

```scala
val input = sc.newAPIHadoopFile(
    inputFile,
    classOf[LzoJsonInputFormat],
    classOf[LongWritable], 
    classOf[MapWritable], 
    conf
)

// "输入"中的每个MapWritable代表一个JSON对象

// 说明：LZO的支持需要我们先安装 hadoop-lzo包，并将其放在Spark的本地库中。如果你使用Debian包安装，在调用spark-submit 是加上 --driver-library-path usr/lib/hadoop/lib/native/ --driver-class-path /usr/lib/hadoop/lib/ 就宽依赖
```



##### **2）保存Hadoop输出格式**

Java API中没有直接将 Pair RDD 保存成SequenceFile 的算子（JavaPairRDD上没有提供saveAsSequenceFile算子）。所以我们要使用自定义Hadoop格式的功能来实现。

我们可以旧式Hadoop 格式的API，也可以通过新式的API（saveAsNewHadoopFile），旧式与新式API在使用上是相识的。

```scala
// 旧式API
def saveAsHadoopFile[F <: OutputFormat[_, _]](
    path: String,
    keyClass: Class[_],
    valueClass: Class[_],
    outputFormatClass: Class[F],
    codec: Class[_ <: CompressionCodec]
): Unit

// 新式API
def saveAsNewAPIHadoopFile(
    path: String,	// 新式API
    keyClass: Class[_],	 // Key类型
    valueClass: Class[_],	// Key类型
    outputFormatClass: Class[_ <: NewOutputFormat[_, _]],  // 输出格式OutputFormat实现
    conf: Configuration = self.context.hadoopConfiguration	 // 配置信息
): Unit 
```

演示：

使用旧式API在 Java中 保存 SequenceFile

```java
public class ConvertToWritableTypes implements PairFunction<Tuple2<String, Integer>, Text, IntWritable> {
    @Override
    public Tuple2<Text, IntWritable> call(Tuple2<String, Integer> record) throws Exception {
        return new Tuple2<>(new Text(record._1), new IntWritable(record._2));
    }
}


public class SparkRDD_sequence {
    public static void main(String[] args) {
        SparkConf sparkConf = new SparkConf().setMaster("local[*]").setAppName("sequence文件");
        JavaSparkContext jsc = new JavaSparkContext(sparkConf);
        Tuple2<String, Integer> tuple = new Tuple2<>("hello", 1);
        Tuple2<String, Integer> tuple2 = new Tuple2<>("world", 1);
        JavaPairRDD<String, Integer> pairRDD = jsc.parallelizePairs(Arrays.asList(tuple, tuple2));
        // 保存数据为sequence
        // todo Java中没有提供专门保存为SequenceFile的算子（在Scala中有saveAsSequenceFile）
        JavaPairRDD<Text, IntWritable> result = pairRDD.mapToPair(new ConvertToWritableTypes());
        String fileName = "省略";  // 文件输出路径
        result.saveAsHadoopFile(fileName, Text.class, IntWritable.class, SequenceFileOutputFormat.class);
    }
}
```

##### 3）非文件系统数据源

除了 hadoopFile() 和 saveAsHadoop() 这两类函数以外，还可以使用**hadoopDataset/ saveAsHadoopDataset 和 newAPIHadoopDataset/saveAsNewAPIHadoopDataset 来访问Hadoop所支持的非文件系统的存储格式。**

**例如：HBase 和 MongoDB等。**



### **2.5 Object文件**

对象文件看起来就像是对sequenceFile 的简单封装，它允许存储只包含值RDD。

和sequenceFile不同，对象文件依赖于Java序列化机制，是使用Java序列化写出来的。

对象文件是将对象序列化后保存的文件，采用 Java 的序列化机制。可以通过objectFile方法接收一个路径，读取对象文件，返回对应的RDD，也可以通过调用saveAsObjectFile()实现对对象文件的输出。因为是序列化所以要指定类型。

```java
public <T> JavaRDD<T> objectFile(final String path) 
public <T> JavaRDD<T> objectFile(final String path, final int minPartitions) 
```

```java
public void saveAsObjectFile(final String path)
```

对象文件在python中无法使用，不过Python中的RDD和SparkContext支持saveAsPickleFile() 和 pickleFile() 方法作为替代。这会使用到Python的序列化库 pickle。



### 2.6 文件压缩

对于大数据来说，我们经常需要对数据进行压缩以节省存储空间和网络传输开销。

对于大多数的Hadoop输出格式来说，我们可以指定一种压缩编码器来压缩数据。Spark原生的输入方式（如 textFile 和 sequenceFile）可以自动处理一些类型的压缩。在读取压缩后的数据时，一些压缩编码器可以推测压缩类型。

这些压缩选型只适用于支持压缩的Hadoop格式，也就是说哪些写出到文件系统的格式。写入数据库的Hadoop格式一般没有实现压缩支持。如果数据库在有压缩过的记录，那应该是数据库自己配置的。

选择一个输出压缩编码器可能会对这些数据以后的用户产生很大的影响。分布式系统中，经常将数据写入到不同的节点，读取数据时也需要从不同的节点上读取数据。要实现这种情况，每个工作节点都必须能找到一条新纪录的开端。有些压缩格式会使这变得不可能，而必须从单个节点来读取所有的数据，这容易产生瓶颈。

**可以从多个节点上并行读取数据的格式称为”可分割“。**

下面是一些压缩选型

| 格式   | 可<br>分<br>割 | 平均压<br>缩速度 | 文本文件<br>压缩效率 | Hadoop压缩编解码器                         | 纯Java实现 | 原生 |
| ------ | -------------- | ---------------- | -------------------- | ------------------------------------------ | ---------- | ---- |
| gzip   | 否             | 快               | 高                   | org.apache.hadoop.io.compress.GzipCodec    | 是         | 是   |
| lzo    | 是             | 非常快           | 中等                 | com.hadoop.compression.lzo.lzoCodeC        | 是         | 是   |
| bzip2  | 是             | 慢               | 非常高               | org.apache.hadoop.io.compress.BzipCodec    | 是         | 是   |
| zlib   | 否             | 慢               | 中等                 | org.apache.hadoop.io.compress.DefaultCodec | 是         | 是   |
| Snappy | 否             | 非常快           | 低                   | org.apache.hadoop.io.compress.SnappyCodec  | 否         | 是   |

说明：

​	lzo：需要在每个节点上安装LZO（是否可分割关键取决于所使用的库）

​	bzip：为可分隔版本使用纯Java

​	zlib：Hadoop默认压缩编码器

​	Snappy：Snppy有纯 Java 的移植版，但是在 Spark/Hadoop中不能使用

注意：

​	尽管Spark的 textFile() 算子可以处理压缩过的输入，但即使输入数据被以可分割读取的方式压缩，Spark也不会打开 splittable。因此，**如果我们要读取单个压缩过的输入，最好不要考虑使用Spark的封装，而是使用newAPIHadoopFIle 或 hadoopFIle ，并且指定正确的压缩编码器**



## 2 文件系统

Spark支持读写很多种文件系统，可以使用任何我们想要的文件格式，比如RDMS表中或HBase表中读写数据，这都是企业中常用的。

### 本地文件系统（一般为Linux本地）

Spark支持从本地文件系统中读取文件，不过它要求文件在集群中的所有节点的相同路径下都可以找到。

例如：

在 Scala中 从本地文件系统读取一个压缩的文本文件

```scala
val rdd = sc.textFile("file://home/nhk/happypands.gz")
```

### HDFS

在Spark中使用HDFS只需要将输入输出路径指定为 hdfs://master:port:/path 即可

### HBase

HBase的使用场景：

-   要分析的数据存储在HBase表中，需要从其中读取数据数据分析。如：日志数据：电商网站的商家操作日志，订单数据：保险行业订单数据等

Spark可以**从HBase表中读写（Read/Write）数据，底层采用TableInputFormat和 TableOutputFormat方式**，与MapReduce与HBase集成完全一样，使用输入格式InputFormat和输出格式OutputFoamt。

-   从HBase表中读取数据 InputForma
    -   加载HBase表中的数据，封装至RDD中
    -   TableInputFormat：<Rowkey, Result>
-   写入数据到HBase表中 OutputFoamt
    -   将RDD数据保存至HBase表中
    -   TableOutputFormat：<Rowkey, Put>

#### **HBase Sink**(读取数据)

回 顾 MapReduce 向 HBase 表 中 写 入 数 据 ， 使 用 TableReducer ， 其 中 OutputFormat 为 TableOutputFormat，读取数据Key：ImmutableBytesWritable，Value：Put。 

-   **写 入 数 据 时 ， 需 要 将 RDD 转换为 RDD[(ImmutableBytesWritable, Put)] 类 型 ， 调 用saveAsNewAPIHadoopFile方法数据保存至HBase表中**。 
-   HBase Client连接时，需要设置依赖Zookeeper地址相关信息及表的名称，通过Configuration设置属性值进行传递

演示：

将词频统计wordcount的保存保存到HBase，表设计如下：

```shell
HBase表设计:
	表名称:	htb_wordcount
	RowKey:		word
	列族:		info
	字段名称:	count	
```

创建HBase表语句：

```shell
hbase(main):002:0> create "htb_wordcount","info"
```

代码如下：

```scala
import org.apache.hadoop.conf.Configuration
import org.apache.hadoop.hbase.HBaseConfiguration
import org.apache.hadoop.hbase.client.Put
import org.apache.hadoop.hbase.io.ImmutableBytesWritable
import org.apache.hadoop.hbase.mapreduce.TableOutputFormat
import org.apache.hadoop.hbase.util.Bytes
import org.apache.spark.rdd.RDD
import org.apache.spark.{SparkConf, SparkContext}

/**
 * 将词频统计的结果（RDD）保存至HBase
 */
object SparkWriterHBase {
  def main(args: Array[String]): Unit = {
    // 创建应用程序入口SparkContext实例
    val sc: SparkContext = {
      val sparkConf = new SparkConf()
        .setAppName(this.getClass.getSimpleName.stripPrefix("$"))
        .setMaster("local[2]")
      new SparkContext(sparkConf)
    }
    // sc.setLogLevel("WARN")
    // 模拟词频统计
    val list = List(("hello", 34), ("word", 45), ("spark", 99))
    val resultRDD: RDD[(String, Int)] = sc.parallelize(list)
    // todo 将数据写入到HBase表中
    // 因为使用saveAsNewAPIHadoopFile函数，要求是 Pair RDD
    // 所有需要组装RDD[(ImmutableBytesWritable, Put)]
    val putsRDD: RDD[(ImmutableBytesWritable, Put)] = resultRDD.mapPartitions { iter =>
      iter.map { case (word, count) =>
        // 创建 RowKey 实例
        val rowKey = new ImmutableBytesWritable(Bytes.toBytes(word))
        // 创建 Put 实例
        val put: Put = new Put(rowKey.get())
        // 添加列
        put.addColumn(
          // 实际项目中使用HBase时，插入数据，先将所有字段的值转为String，再使用Bytes转换为字节数组
          Bytes.toBytes("info"),  // 列簇
          Bytes.toBytes("count"),   // 列限定符
          Bytes.toBytes(count.toString)   // value
        )
        // 返回二元组
        (rowKey, put)
      }
    }
    // 构建 HBase Client 配置信息
    val conf: Configuration = HBaseConfiguration.create()
    // 设置连接的 ZK 属性
    conf.set("hbase.zookeeper.quorum", "kk01")
    conf.set("hbase.zookeeper.property.clientPort", "2181")
    conf.set("zookeeper.znode.parent", "/hbase")
    // 设置将数据保存的HBase表的名称
    conf.set(TableOutputFormat.OUTPUT_TABLE, "htb_wordcount")

    // todo 保存数据到HBase表中
    putsRDD.saveAsNewAPIHadoopFile(
      "datas/spark/htb-output-" + System.nanoTime(), // 文件输出路径
      classOf[ImmutableBytesWritable],	// rowKey
      classOf[Put],  // put
      classOf[TableOutputFormat[ImmutableBytesWritable]],
      conf
    )

    // 应用程序运行结束，关闭资源
    sc.stop()
  }
}
```

使用hbase shell查看数据：

```shell
hbase(main):006:0> scan "htb_wordcount"
ROW                                      COLUMN+CELL                                                                                                         
 hello                                   column=info:count, timestamp=2023-08-08T16:30:04.873, value=34                                                      
 spark                                   column=info:count, timestamp=2023-08-08T16:30:04.873, value=99                                                      
 word                                    column=info:count, timestamp=2023-08-08T16:30:04.873, value=45                                                      
3 row(s)
Took 0.0487 seconds                                                                                       
```

#### **HBase Source**(保存数据)

回 顾 MapReduce 从 读 HBase 表 中 的 数 据 ， 使 用 TableMapper ， 其 中 InputFormat 为TableInputFormat，读取数据Key：ImmutableBytesWritable，Value：Result。 

从HBase表读取数据时，同样需要设置依赖Zookeeper地址信息和表的名称，使用Configuration设置属性

此外，读取的数据封装到RDD中，Key和Value类型分别为：ImmutableBytesWritable和Result

**ImmutableBytesWritable 、Result 不支持Java Serializable导致处理数据时报序列化异常**。设置Spark Application使用Kryo序列化，性能要比Java 序列化要好，创建SparkConf对象设置相关属性

演示：

从HBase表读取词频统计结果

```scala
import org.apache.hadoop.conf.Configuration
import org.apache.hadoop.hbase.{CellUtil, HBaseConfiguration}
import org.apache.hadoop.hbase.client.Result
import org.apache.hadoop.hbase.mapreduce.TableInputFormat
import org.apache.hadoop.hbase.io.ImmutableBytesWritable
import org.apache.hadoop.hbase.util.Bytes
import org.apache.spark.rdd.RDD
import org.apache.spark.{SparkConf, SparkContext}

/**
 * 从 HBase 表中读取数据，封装到RDD中
 */
object SparkReadHBase {
  def main(args: Array[String]): Unit = {
    // 创建应用程序入口SparkContext实例
    val sc: SparkContext = {
      val sparkConf = new SparkConf()
        .setAppName(this.getClass.getSimpleName.stripPrefix("$"))
        .setMaster("local[2]")
        // todo 设置使用 Kryo 序列化方式：因为 ImmutableBytesWritable, Result 不支持Java序列化
        .set("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
        // todo 注册序列化的数据类型
        .registerKryoClasses(Array(classOf[ImmutableBytesWritable], classOf[Result]))
      new SparkContext(sparkConf)
    }
//    sc.setLogLevel("WARN")

    // TODO 读取 HBase Client 配置信息
    // 构建 HBase Client 配置信息
    val conf: Configuration = HBaseConfiguration.create()
    // 设置连接的 ZK 属性
    conf.set("hbase.zookeeper.quorum", "kk01")
    conf.set("hbase.zookeeper.property.clientPort", "2181")
    conf.set("zookeeper.znode.parent", "/hbase")
    // TODO 设置读取的表的名称
    conf.set(TableInputFormat.INPUT_TABLE, "htb_wordcount")

    // todo 读取HBase表数据到RDD
    val resultRDD: RDD[(ImmutableBytesWritable, Result)] = sc.newAPIHadoopRDD(
      conf,
      classOf[TableInputFormat],
      classOf[ImmutableBytesWritable], // rowKey
      classOf[Result] // result
    )
    println(s"Count = ${resultRDD.count()}")

    resultRDD.
      take(5) // 该算子会产生网络传输，所以数据需要序列化
      .foreach { case (rowKey, result) =>
        println(s"RowKey = ${Bytes.toString(rowKey.get())}")
        // HBase表中的每条数据封装在result对象中，解析获取每列的值
        result.rawCells().foreach(cell => {
          val cf = Bytes.toString(CellUtil.cloneFamily(cell))
          val column = Bytes.toString(CellUtil.cloneQualifier(cell))
          val value = Bytes.toString(CellUtil.cloneValue(cell))
          val version = cell.getTimestamp
          println(s"\t $cf:$column = $value, version = $version")
        }
        )
      }
    // 应用程序运行结束，关闭资源
    sc.stop()
  }
}
```



###  MySQL

使用场景：

-   使用Spark进行离线分析以后，往往将报表结果保存到MySQL表中。如：网站基本分析（pv、uv....）

实际开发中常常将分析结果RDD保存至MySQL表中，使用`foreachPartition`函数

此外Spark中提供 `JdbcRDD`用于从MySQL表中读取数据。 

调用 foreachPartition函数将每个分区数据保存至MySQL表中，保存时考虑降低RDD分区数目和批量插入，提升程序性能

演示：

将词频统计WordCount结果保存MySQL表tb_wordcount

建表语句：

```mysql
CREATE DATABASE if not exists db_test;
USE db_test ;
CREATE TABLE `tb_wordcount` (
    `count` varchar(100) NOT NULL,
    `word` varchar(100) NOT NULL,
    PRIMARY KEY (`word`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;
```

代码：

```scala
package com.clear.spark.io.mysql

import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.rdd.RDD

import java.sql.{Connection, Driver, DriverManager, PreparedStatement}

/**
 * 将词频统计结果保存到MySQL表中
 */
object SparkWriteMySQL {
  def main(args: Array[String]): Unit = {
    // 创建应用程序入口SparkContext实例
    val sc: SparkContext = {
      val sparkConf = new SparkConf()
        .setAppName(this.getClass.getSimpleName.stripPrefix("$"))
        .setMaster("local[2]")
      new SparkContext(sparkConf)
    }
    // 模拟词频统计
    val list = List(("hello", 34), ("word", 45), ("spark", 99))
    val resultRDD: RDD[(String, Int)] = sc.parallelize(list)
    // todo 输出结果RDD保存到MySQL数据库
    resultRDD
      // 降低RDD分区数目：将结果RDD保存到外部存储系统时，可以考虑
      .coalesce(1)
      // 对分区数据操作
      .foreachPartition(iter => saveToMysql(iter))

    // 应用程序运行结束，关闭资源
    sc.stop()
  }

  /**
   * 将RDD的每个分区的数据保存到MySQL数据库表中
   *
   * @param datas 迭代器，封装RDD中每个分区的数据
   */
  def saveToMysql(datas: Iterator[(String, Int)]) = {
    // 注册驱动
    Class.forName("com.mysql.jdbc.Driver") // mysql8 ==> com.mysql.cj.jdbc.Driver

    var connection: Connection = null
    var statement: PreparedStatement = null

    try {
      // todo 获取数据库连接
      connection = DriverManager.getConnection(
        "jdbc:mysql://192.168.188.128:3306/?serverTimezone=UTC&characterEncoding=utf8&useUnicode=true",
        "root",
        "123456")
      // todo 获取SQL执行对象
      val sql = "INSERT INTO db_test.tb_wordcount (word, count) VALUES(?, ?)"
      statement = connection.prepareStatement(sql)
      // 获取当前数据库事务
      val autoCommit:Boolean = connection.getAutoCommit
      connection.setAutoCommit(false)

      // 将分区中数据插入到表中，批量插入
      datas.foreach {
        case (word, count) =>
          statement.setString(1, word)
          statement.setLong(2, count)
          // 加入批次
          statement.addBatch()
      }
      // todo 执行SQL：批量插入
      statement.executeBatch()

      connection.commit() // 手动提交事务
      connection.setAutoCommit(autoCommit)  // 还原数据库原来的事务
    } catch {
      case e: Exception => e.printStackTrace()
    } finally {
      // todo 释放资源
      if (null != statement) statement.close()
      if (null != connection) connection.close()
    }
  }
}
```

结果：

```mysql
mysql> select * from tb_wordcount;
+-------+-------+
| count | word  |
+-------+-------+
| 34    | hello |
| 99    | spark |
| 45    | word  |
+-------+-------+
3 rows in set (0.00 sec)
```

#### RDD数据保存到MySQL核心代码

```scala
// 获取当前数据库事务
val autoCommit:Boolean = connection.getAutoCommit
connection.setAutoCommit(false)

// 将分区中数据插入到表中，批量插入
datas.foreach {
    case (word, count) =>
    statement.setString(1, word)
    statement.setLong(2, count)
    // 加入批次
    statement.addBatch()
}
// todo 执行SQL：批量插入
statement.executeBatch()

connection.commit() // 手动提交事务
connection.setAutoCommit(autoCommit)  // 还原数据库原来的事务
```



# 3 数据库与键值存储

## Java数据库来连接

Spark可以支持任何Java数据库连接（JDBC）的关系型数据库中读取数据，例如 Mysql、Postgre系统等。

要访问数据库的信息，我们需要使用 JdbcRDD（org.apache.spark.rdd.JdbcRDD）

```scala
class JdbcRDD[T: ClassTag](
    sc: SparkContext,	
    getConnection: () => Connection,	// 创建数据库连接
    sql: String,
    //  lowerBound upperBound 为查询参数
    lowerBound: Long,
    upperBound: Long,
    numPartitions: Int,
    mapRow: (ResultSet) => T = JdbcRDD.resultSetToObjectArray _)	// 用于将输出结果从ResultSet转为对操作数据有用的格式的函数；如果该参数空缺，Spark会自动将每行结果转换为对象数组
  extends RDD[T](sc, Nil) with Logging
```

演示：

在Scala中使用JdbcRDD

```scala
def createConnection() = {
    // 注册驱动
    Class.forName("com.mysql.jdbc.Driver").newInstance()
    // 获取数据库连接
    DriverManager.getConnection("jbdc:mysql://localhost/test?user=root")
}

def extractValues(r: ResultSet) = {
    (r.getInt(1), r.getString(2))
}


val data = new JdbcRDD(sc,
                       createConnection(), 
                       "select * from panda where ? < id and id <= ?",
                       lowerBound = 1,
                       upperBound = 3,
                       numPartitions = 2,
                       mapRow = extractValues
                      )
println(data.collect().toList)
```



## HBase

由于 `org.apache.hadoop.hbase.mapreduce.TableInputFormat` 类的实现。Spark可以通过Hadoop输入格式访问HBase。

这个输入格式会返回键值对的数据，其中键为 `org.apache.hadoop.hbase.io.ImmutableBytesWritable`，值为`org.apache.hadoop.habse.client.Result`。Result类包含多种根据列获取值的方法。

Result 类API 文档https://hbase.apache.org/apidocs/org/apche/hadoop/hbase/client/Result.html 

**要将Spark用于Hbase，我们需要在 sc.newAPIHadoopRDD 中传入正确的输入格式**

演示：

从Hbase读取数据

```scala
import org.apache.hadoop.hbase.HBaseConfiguration
import org.apache.hadoop.hbase.client.Result
import org.apache.hadoop.hbase.io.ImmutableBytesWritable
import org.apache.hadoop.hbase.mapreduce.TableInputFormat
import org.apache.hadoop.hbase.HBaseConfiguration



val conf = HBaseConfiguration.create()
conf.set(TableInputFormat.INPUT_TABLE,"tablename")  // 扫描哪种表

val rdd = sc.newAPIHadoopRDD(
    conf,classOf[TableInputFormat],classOf[ImmutableWritable],classOf[Result]
)
```

其中：

​	TableInputFormat包含多个可以用来优化HBase的读取的设置项，比如将扫描限制到部分列，以及限制扫描的时间范围等



## Elasticsearch

Elasticsearch是一个开源的、基于Lucene的搜索系统。

Spark可以使用 Elasticsearch-Hadoop（https://github.com/elastic/elasticsearch-hadoop）从Elasticsearch中读取数据。

演示：

在Scala 中使用Elasticsearch输出

```scala
val jobConf = new JobConf(sc.hadoopConfiguration)
jobCOnf.set("mapred.output.format.class", "org.elasticsearch.hadoop.mr.EsOutputFormat")
jobConf.setOutputCommitter(classOf[FileOutputCommitter])
jobConf.set(ConfigurationOptions.ES_RESOURCE_WRITE,"twitter/tweets")
jobConf.set(ConfigurationOptions.ES_NODES,new Path("-"))
output.saveAsHadoopDataset(jobConf)      
```

在Scala 中使用Elasticsearch输入

```scala
def mapWritableToInput(in: MapWritable): Map[String, String] = {
    in.map{case (k,v) => (k.toString, v.toString)}.toMap
}

val jobConf = new JobConf(sc.hadoopConfiguration)
jobConf.set(ConfigurationOptions.ES_RESOURCE_READ,args(1))
jobConf.set(ConfigurationOptions.ES_NODES,args(2))
val currentTweets = sc.hadoopRDD(jobConf,
                                ClassOf[EsInputFormat[Object, MapWritable]],
                                classOf[Object],
                                classOf[MapWritable])
// 仅提取Map
// 将MapWritable[Text, Text]转为Map[String, String]
val tweets = currentTweets.map{ case (key, value) => mapWritableToInput(value) }    
```



# Spark SQL 中的结构化数据源

在SparkSQL模块，提供一套完成API接口，用于方便读写外部数据源的的数据（从Spark 1.4版本提供），框架本身内置外部数据源：如csv、jdbc、json、orc、parquet、text

在Spark 2.4版本中添加支持 Image Source（图像数据源）和 Avro Source。

在读取各种数据源的情况下，对数据源的执行查询，然后得到Row对象组成的RDD，每个Row对象表示一个记录。

在 Java 和 Scala 中，**Row对象的访问是基于下标的**。每个Row 都会有get() 方法（读取到的数据一般会进行数据类型转换）。此外，常见的数据类型还会有专门的get方法，例如：getFloat()、getInt()、getLong()、getString()等等。（**个人比较喜欢使用getAs(String fieldName)的方法读取，不易出错**）

在Python中，可以使用row[column_number] 以及 row.column_name 来访问元素

## 数据源格式

数据分析处理中，数据可以分为结构化数据、非结构化数据及半结构化数据

-   结构化数据（Structured）
    -   结构化数据源可提供有效的存储和性能。例如，Parquet和ORC等柱状格式使从列的子集中提取值变得更加容易。 
    -   基于行的存储格式（如Avro）可有效地序列化和存储提供存储优势的数据。然而，这些优点通常以灵活性为代价。如因结构的固定性，格式转变可能相对困难

-   非结构化数据（UnStructured） 
    -   相比之下，非结构化数据源通常是自由格式文本或二进制对象，其不包含标记或元数据以定义数据的结构。 
    -   报纸文章，医疗记录，图像，应用程序日志通常被视为非结构化数据。这些类型的源通常要求数据周围的上下文是可解析的。 

-   半结构化数据（Semi-Structured） 
    -   半结构化数据源是按记录构建的，但不一定具有跨越所有记录的明确定义的全局模式。每个数据记录都使用其结构信息进行扩充。 

-   半结构化数据格式的好处是，它们在表达数据时提供了最大的灵活性，因为每条记录都是自我描述的。但这些格式的主要缺点是它们会产生额外的解析开销，并且不是特别为 ad-hoc(特定)查询而构建的。



## 数据的加载与保存

SparkSQL提供一套通用外部数据源接口，方便用户从数据源加载和保存数据（这里的通用指的是使用相同的API，根据不同的参数读取和保存不同格式的数据，SparkSQL**默认读取和保存的文件格式为parquet**）

例如从MySQL 表中既可以加载读取数据：load/read，又可以保存写入数据：save/write。 

```scala
// 从 JDBC source 中加载数据
val jdbcDF = spark.read
	.format("jdbc")
	.option("url","jdbc:postgresql:dbserver")
	.option("datable","schame.tablename")
	.option("user","username")
	.option("password","password")
	.load()

// 保存数据到 JDBC source
jdbcDF.writer
	.format("jdbc")
	.option("url","jdbc:postgresql:dbserver")
	.option("datable","schame.tablename")
	.option("user","username")
	.option("password","password")
	.save()
```

由于SparkSQL没有内置支持从HBase表中加载和保存数据，但是只要实现外部数据源接口，也能像上面方式一样读取加载数据。 

### 1）加载数据

在SparkSQL中读取数据使用SparkSession读取，并且封装到数据结构Dataset/DataFrame中

```scala
/**
 * Returns a [[DataFrameReader]] that can be used to read non-streaming data in as a
 * `DataFrame`.
 * {{{
 *   sparkSession.read.parquet("/path/to/file.parquet")
 *   sparkSession.read.schema(schema).json("/path/to/file.json")
 * }}}
 *
 * @since 2.0.0
 */
def read: DataFrameReader = new DataFrameReader(self)
```

DataFrameReader专门用于加载load读取外部数据源的数据，基本格式如下：

**spark.read.load** 是加载数据的**通用方法**

```scala
scala> spark.read.

csv   format   jdbc   json   load   option   options   orc   parquet   schema   table   text   textFile

// load是加载(读取)数据的通用方法
// 其他的则是读取不同格式数据的特定方法（其中 parquet是SparkSQL默认的读取和保存格式）
```

所有读取 API 遵循以下调用格式：

```scala
// 格式
spark.read					// DataFrameReader专门用户加载外部数据源的数据
	// 指定加载的数据类型（数据源）
	// 包括"csv"、"jdbc"、"json"、"orc"、"es"、"parquet"和"textFile"等
	.format("...")
	// 可选的，设置读取的数据的schema信息
	.schema(...)
	// 数据源的参数选项配置，例如，在"jdbc"格式下需要传入JDBC相应参数，url、user、password和dbtable
	.option("key", "value")   // 可以配置多个
	// 在"csv"、"jdbc"、"json"、"orc"、"parquet"和"textFile"格式下需要传入加载数据的路径。
	.load("...")
	
 
// 示例  spark.read.load 是加载数据的通用方法
spark.read
	.format("csv")
	.schema(someSchema)                  // 使用预定义的 schema      
	.option("mode", "FAILFAST")          // 读取模式
	.option("inferSchema", "true")       // 是否自动推断 schema
	.option("path", "path/to/file(s)")   // 文件路径
	.load()
```

如果读取不同格式的数据，可以对不同的数据格式进行设定

```scala
scala> spark.read.format("…")[.option("…")].load("…")
```

我们前面都是使用read API 先把文件加载到 DataFrame然后再查询，

此外加载文件数据时，可以直接使用SQL语句，指定文件存储格式和路径（路径需要使用反引号`包裹起来）：

```scala
scala> 
       spark.sql("select * from json.`/opt/temp/spark/user.json`").show
warning: 1 deprecation (since 2.13.3); for details, enable `:setting -deprecation` or `:replay -deprecation`
23/05/09 01:25:52 WARN ObjectStore: Failed to get database json, returning NoSuchObjectException
+---+--------+                                                                  
|age|username|
+---+--------+
| 20|zhangsan|
| 29|    lisi|
| 25|  wangwu|
| 30| zhaoliu|
| 35|  tianqi|
| 40|   jerry|
+---+--------+
```

#### 读取模式

读取模式有以下几种

| mode          | 描述                                                         |
| ------------- | ------------------------------------------------------------ |
| permissive    | 当遇到损坏的记录时，将其所有字段设置为 null，并将所有损坏的记录放在名为 _corruption t_record 的字符串列中 |
| dropMalformed | 删除格式不正确的行                                           |
| failFast      | 遇到格式不正确的数据时立即失败                               |

### 2）保存数据

SparkSQL模块中可以从某个外部数据源读取数据，就能向某个外部数据源保存数据，提供相应接口，通过DataFrameWrite类将数据进行保存。

```scala
/**
 * Interface for saving the content of the non-streaming Dataset out into external storage.
 *
 * @group basic
 * @since 1.6.0
 */
def write: DataFrameWriter[T]
```

与DataFrameReader类似，提供一套规则，将数据Dataset保存，基本格式如下：

```scala
// 格式
dataset/dataframe.writer		
	// 表示保存数据模型
	.mode(SaveMode)
	// 指定保存的数据类型，包括"csv"、"jdbc"、"json"、"orc"、"parquet"和"textFile"
	.format("")
	// 保存数据源可选项。例如，在"jdbc"格式下需要传入JDBC相应参数，url、user、password和dbtable
	.option("","")
	// 保存数据：在"csv"、"orc"、"parquet"和"textFile"格式下需要传入 保存数据的路径
	.save()


//示例
dataframe
	.write
	.format("csv")
	.option("mode", "OVERWRITE")         //写模式
	.option("dateFormat", "yyyy-MM-dd")  //日期格式
	.option("path", "path/to/file(s)")
	.save()
```

保存操作可以使用 **SaveMode**, 用来指明如何处理数据，使用**mode()方法来设置**。例如，

```scala
df.write.mode("append").json("/opt/temp/output")
```

有一点很重要: 这些 **SaveMode 都是没有加锁的, 也不是原子操作**。

使用SpakrSQL分析数据时，从数据读取，到数据分析及数据保存，链式操作，更多就是 ETL操作。当将结果数据DataFrame/Dataset保存至Hive表中时，可以设置分区partition和分桶bucket，形式如下：

```scala
// 保存数据至表，可以设置分区和分桶，就是Hive的分区表和分桶表
dataframe.write
	.partitionBy("")	// 分区列名称
	.bucketBy(10, "")	// 分桶的列名称和数目
	.saveAsTable("")	// 表的名称
```



#### 保存模式（SaveMode）

将Dataset/DataFrame数据保存到外部存储系统中，考虑是否存在，存在的情况下的下如何进行保存，DataFrameWriter中有一个mode方法指定模式：

```scala
/**
   * Specifies the behavior when data or table already exists. Options include:
   * <ul>
   * <li>`SaveMode.Overwrite`: overwrite the existing data.</li>
   * <li>`SaveMode.Append`: append the data.</li>
   * <li>`SaveMode.Ignore`: ignore the operation (i.e. no-op).</li>
   * <li>`SaveMode.ErrorIfExists`: throw an exception at runtime.</li>
   * </ul>
   * <p>
   * The default option is `ErrorIfExists`.
   *
   * @since 1.4.0
   */
def mode(saveMode: SaveMode): DataFrameWriter[T] = {
    this.mode = saveMode	// 其中 SaveMode是Java语言编写的枚举类
    this
}
```

SaveMode是一个枚举类，使用Java语言编写，如下四种保存模式（常量）：

| Scala/Java                          | Any Language        | 描述                             |
| ----------------------------------- | ------------------- | -------------------------------- |
| SaveMode.ErrorIfExists    (default) | "error"   (default) | 如果文件已经存在则抛出异常       |
| SaveMode.Append                     | "append"            | 追加模式；如果文件已经存在则追加 |
| SaveMode.Overwrite                  | "overwrite"         | 覆写模式；如果文件已经存在则覆盖 |
| SaveMode.Ignore                     | "ignore"            | 忽略；如果文件已经存在则忽略     |

注意：

​	在生产环境中，保存模式慎用。比如说往数据库表中写入数据时，我们就不用保存模式，而是自己手写RDD代码



## 操作Parquet

-   **Spark SQL的默认数据源为Parquet格式**。Parquet是一种能够有效存储嵌套数据的列式存储格式。
-   **数据源为Parquet文件时**，Spark SQL可以方便的执行所有的操作，**不需要使用format**。
    -   spark SQL 提供了直接读取和存储 Parquet格式文件的方法
-   修改配置参数 `spark.sql.sources.default`，可修改默认数据源格式。

### 1）加载数据

数据准备

在 /opt/software/spark-local/examples/src/main/resources 目录下有个users.parquet 文件，我们先将它拷贝到/opt/temp/saprk目录下

```shell
[root@kk01 resources]# cp /opt/software/spark-local/examples/src/main/resources/users.parquet /opt/temp/spark/
```

```scala
// 加载数据，parquet格式数据是SparkSQL默认的数据源格式，因此不需要使用format()
scala> var df = spark.read.load("/opt/temp/spark/users.parquet")
var df: org.apache.spark.sql.DataFrame = [name: string, favorite_color: string ... 1 more field]

scala> df.show
warning: 1 deprecation (since 2.13.3); for details, enable `:setting -deprecation` or `:replay -deprecation`
+------+--------------+----------------+
|  name|favorite_color|favorite_numbers|
+------+--------------+----------------+
|Alyssa|          null|  [3, 9, 15, 20]|
|   Ben|           red|              []|
+------+--------------+----------------+
```

```scala
// todo 读取parquet数据
val userDF: DataFrame = spark.read.parquet("datas/resources/user.parquet")
// 使用默认的数据源加载数据（spark中默认数据源就是 parquet）
val userDF2: DataFrame = spark.read.load("datas/resources/user.parquet")
```

### 2）保存数据

```scala
// 也可以使用 spark.read.json("/opt/temp/spark/user.json") 代替下面命令
scala> var df = spark.read.format("json").load("/opt/temp/spark/user.json")
var df: org.apache.spark.sql.DataFrame = [age: bigint, username: string]

// 保存为parquet格式
scala> df.write.mode("append").save("/opt/temp/output")

// 查看
[root@kk01 output]# pwd
/opt/temp/output
[root@kk01 output]# ll
total 4
-rw-r--r--. 1 root root 809 May  9 02:13 part-00000-6a0b7dfc-25eb-4014-a442-146fb4e83df9-c000.snappy.parquet
-rw-r--r--. 1 root root   0 May  9 02:13 _SUCCESS
```

## 操作text

SparkSession加载文本文件数据，提供两种方法，返回值分别为DataFrame和Dataset

方法声明如下：

```scala
// 返回值DataFrame
def text(paths: String*): DataFrame = format("text").load(paths : _*)

// 返回值Dataset
def textFile(paths: String*): Dataset[String] = {
    // 调用的还是text方法，使用as[String]转换为Dataset
    text(paths : _*).select("value").as[String]
}
```

可以看出textFile方法底层还是调用text方法，先加载数据封装到DataFrame中，再使用as[String] 方法将DataFrame转换为Dataset，实际项目中推荐使用textFile方法，从Spark 2.0开始提供。 

无论是text方法还是textFile方法读取文本数据时，**一行一行的加载数据，每行数据使用UTF-8编码的字符串，列名称为【value】**

演示：

数据准备：hdfs://kk01:8020/opt/temp/spark/person.txt

```
1 zhangsan 90
2 lishi 100
3 wanwu 20
```

代码编写

```scala
import org.apache.spark.sql.{DataFrame, Dataset, SparkSession}

/**
 * SparkSQL加载文本数据，分别使用text、textFile方法
 *
 */
object SparkSQLText {
    def main(args: Array[String]): Unit = {
        // todo 构建SparkSession实例对象，通过建造者模式创建
        val spark: SparkSession = SparkSession.builder()
          .appName("SparkSQL加载文本数据")
          .master("local[3]")
          .getOrCreate()   // 底层实现：单例模式，创建SparkContext对象
        import spark.implicits._
        // todo text方法加载数据，封装至DataFrame中
        val dataframe: DataFrame = spark.read.text("hdfs://kk01:8020/opt/temp/spark/person.txt")
        dataframe.printSchema()
        dataframe.show(10, truncate = false)
        println("=================================================")
        // todo textFile方法加载数据，封装至DataFrame中
        val dataset: Dataset[String] = spark.read.textFile("hdfs://kk01:8020/opt/temp/spark/person.txt")
        dataset.printSchema()
        dataset.show(10, truncate = false)

        // todo 应用结束，关闭资源
        spark.stop()
    }
}
```

运行结果：

```diff
root
 |-- value: string (nullable = true)
 
+-------------+
|value        |
+-------------+
|1 zhangsan 90|
|2 lishi 100  |
|3 wanwu 20   |
+-------------+

=================================================

root
 |-- value: string (nullable = true)
 
+-------------+
|value        |
+-------------+
|1 zhangsan 90|
|2 lishi 100  |
|3 wanwu 20   |
+-------------+
```



## 操作JSON

-   **Spark SQL 能够自动推测 JSON** 数据集的结构，并将它**加载为一个Dataset[Row]**
-   可以通过SparkSession.read.json()去加载JSON 文件。

注意：

​	**Spark读取的JSON文件不是传统的JSON文件，每一行都应该是一个JSON串**。格式如下：

```json
{"name":"Michael"}
{"name":"Andy"， "age":30}
[{"name":"Justin"， "age":19},{"name":"Justin"， "age":19}]
```

​	**Spark读取JSON文件默认不支持一条数据记录跨越多行** (如下)，可以通过**配置 multiLine 为 true 来进行更改**，其默认值为 false。

```json
// 默认支持单行
{"DEPTNO": 10,"DNAME": "ACCOUNTING","LOC": "NEW YORK"}
 
//默认不支持多行
{
  "DEPTNO": 10,
  "DNAME": "ACCOUNTING",
  "LOC": "NEW YORK"
}
```

### 1）加载JSON文件

导入隐式转换

隐射转换在spark-shell中默认已经导入了，在Scala中需要手动导入，但是在Java中，不存在隐式转换

```scala
import spark.implicits._
```

加载JSON文件

```scala
// 加载json文件，加载模式 FAILFAST
scala> var df = spark.read.format("json").option("mode","FAILFAST").load("/opt/temp/spark/user.json")
var df: org.apache.spark.sql.DataFrame = [age: bigint, username: string]

// 查询数据
scala> df.show()
+---+--------+
|age|username|
+---+--------+
| 20|zhangsan|
| 29|    lisi|
| 25|  wangwu|
| 30| zhaoliu|
| 35|  tianqi|
| 40|   jerry|
+---+--------+

// 创建临时表（用于SQL风格的数据操作）
scala> df.createOrReplaceTempView("user")

// 数据查询
scala> var result = spark.sql("select * from user where age >= 29")
var result: org.apache.spark.sql.DataFrame = [age: bigint, username: string]

scala> result.show()
+---+--------+
|age|username|
+---+--------+
| 29|    lisi|
| 30| zhaoliu|
| 35|  tianqi|
| 40|   jerry|
+---+--------+
```

```scala
// todo 读取json数据
val userDF3: DataFrame = spark.read.load("/opt/temp/spark/user.json")

// todo 以文本形式加载json数据
// 使用textFile加载数据，对每条JSON格式字符串数据，使用SparkSQL函数库functions中自带get_json_obejct函数提取字段
val jsonDF = spark.read.text("/opt/temp/spark/user.json")
// TODO：使用SparkSQL自带函数，针对JSON格式数据解析的函数
import org.apache.spark.sql.functions._
// 获取如下两个字段的值：age、username
val gitDF: DataFrame = jsonDF.select(
    get_json_object($"value", "$.age").as("id"),
    get_json_object($"value", "$.username").as("type"),
)
```

### 2）写入JSON文件

```scala
// 将查询出来的结果保存成新的json文件
// 保存模式：若已存在则忽略
scala> result.write.format("json").mode("ignore").save("/opt/temp/user_out")

// 查看
[root@kk01 user_out]# pwd
/opt/temp/user_out
[root@kk01 user_out]# ll
total 4
-rw-r--r--. 1 root root 122 May 13 05:02 part-00000-70077ee1-fae8-4444-98f1-3a8c755be89c-c000.json
-rw-r--r--. 1 root root   0 May 13 05:02 _SUCCESS
[root@kk01 user_out]# cat part-00000-70077ee1-fae8-4444-98f1-3a8c755be89c-c000.json 
{"age":29,"username":"lisi"}
{"age":30,"username":"zhaoliu"}
{"age":35,"username":"tianqi"}
{"age":40,"username":"jerry"}
```

### 3）Java API 

思路：

​	1、注意json文件中的json数据不能是嵌套格式

​	2、Dataset是一个个 Row 类型的RDD

​	3、如果使用SQL风格需要注册临时表

```java
public class SparkSQL_Json {
    public static void main(String[] args) {
        SparkConf sparkConf = new SparkConf().setAppName("加载JSON文件~~~").setMaster("local[*]");
        JavaSparkContext jsc = new JavaSparkContext(sparkConf);
        jsc.setLogLevel("ERROR");  // 设置日志打印级别
        SparkSession spark = SparkSession
                .builder()
                .config(sparkConf)
                .getOrCreate();
        SQLContext sqlContext = new SQLContext(spark);

        // 加载json文件
        Dataset<Row> df = spark.read().json("/opt/temp/spark/user.json");
        // 或
        Dataset<Row> df2 = spark.read().format("json").load("/opt/temp/spark/user.json");

        df.cache();
        // 展示加载的数据
        df.show();

        // 将 DataFrame对象的数据保存成 JSON文件
        // 保存模式：如果文件已经存在则忽略
        df.write().format("json").mode(SaveMode.Ignore).save("/opt/temp/user_test");

        spark.stop();
        jsc.stop();
    }
}
```

打包上传至服务器运行结果如下

```shell
[root@kk01 temp]# /opt/software/spark-local/bin/spark-submit --class com.clear.io.SparkSQL_Json --master local /opt/temp/spark-sql-demo-1.0.jar

+---+--------+
|age|username|
+---+--------+
| 20|zhangsan|
| 29|    lisi|
| 25|  wangwu|
| 30| zhaoliu|
| 35|  tianqi|
| 40|   jerry|
+---+--------+
```

```shell
# 查看生成的json文件
[root@kk01 user_test]# pwd
/opt/temp/user_test
[root@kk01 user_test]# ll
total 4
-rw-r--r--. 1 root root 186 May 13 05:08 part-00000-c260a869-f56a-4aab-9afe-05412f795f98-c000.json
-rw-r--r--. 1 root root   0 May 13 05:08 _SUCCESS
[root@kk01 user_test]# cat part-00000-c260a869-f56a-4aab-9afe-05412f795f98-c000.json 
{"age":20,"username":"zhangsan"}
{"age":29,"username":"lisi"}
{"age":25,"username":"wangwu"}
{"age":30,"username":"zhaoliu"}
{"age":35,"username":"tianqi"}
{"age":40,"username":"jerry"}
```



## 操作CSV

-   Spark SQL可以配置CSV文件的列表信息，读取CSV文件，**CSV文件的第一行设置为数据列**

-   在机器学习中，常常使用的数据存储在csv/tsv文件格式中，所以SparkSQL中也支持直接读取 

    格式数据，从2.0版本开始内置数据源。关于CSV/TSV格式数据说明：

    ```
    CSV数据格式
    	每行数据中各个字段的值使用通常逗号隔开，可以使用Excel打开
    	比如：
    		10001,zhangsan,25,male
    
    TSV数据格式
    	每行数据中各个字段的值使用制表符隔开
    	比如:
    		10001	zhangsan	25	male
    ```

SparkSQL中读取CSV格式数据，可以设置一些选项，重点选项： 

-   1）分隔符：sep 
    -   默认值为逗号，必须单个字符 
-   2）数据文件首行是否是列名称：header  
    -   默认值为false，如果数据文件首行是列名称，设置为true 
-   3）是否自动推断每个列的数据类型：inferSchema  
    -   默认值为false，可以设置为true

### 1）加载CSV文件

数据准备

在 Linux的/opt/temp/spark目录下，有 user.csv文件

```
id,name,age
1,zhangsan,18
2,lisi,19
3,wnagwu,20
```

加载CSV文件

```scala
scala> val df = spark.read.csv("/opt/temp/spark/user.csv")
val df: org.apache.spark.sql.DataFrame = [_c0: string, _c1: string ... 1 more field]

scala> df.show()
+---+--------+---+
|_c0|     _c1|_c2|
+---+--------+---+
| id|    name|age|
|  1|zhangsan| 18|
|  2|    lisi| 19|
|  3|  wnagwu| 20|
+---+--------+---+
```

当读取CSV/TSV格式数据文件首行是否是列名称，读取数据方式（参数设置）不一样的

-   首行是列的名称，如下方式读取数据文件

```scala
// TODO: 读取TSV格式数据
val ratingsDF: DataFrame = spark.read
	// 设置每行数据各个字段之间的分隔符， 默认值为 逗号
    .option("sep", "\\t")
    // 设置数据文件首行为列名称，默认值为 false
    .option("header", "true")
    // 自动推荐数据类型，默认值为false
    .option("inferSchema", "true")
    // 指定文件的路径
    .csv("datas/ml-100k/u.dat")
ratingsDF.printSchema()
ratingsDF.show(10, truncate = false)
```

首行不是列的名称，如下方式读取数据（设置Schema信息）

```scala
// 定义Schema信息
val schema = StructType(
    StructField("user_id", IntegerType, nullable = true) ::
    StructField("movie_id", IntegerType, nullable = true) ::
    StructField("rating", DoubleType, nullable = true) ::
    StructField("timestamp", StringType, nullable = true) :: Nil
)
// TODO: 读取TSV格式数据
val mlRatingsDF: DataFrame = spark.read
	// 设置每行数据各个字段之间的分隔符， 默认值为 逗号
	.option("sep", "\\t")
	// 指定Schema信息
	.schema(schema)
	// 指定文件的路径
	.csv("datas/ml-100k/u.data")
mlRatingsDF.printSchema()
mlRatingsDF.show(5, truncate = false)
```



### 2）保存CSV文件

```shell
// 将查询出来的结果保存成新的csv文件
// 保存模式：若已存在则忽略
scala> df.write.format("csv").mode("ignore").save("/opt/temp/csvout")

// 查看
[root@kk01 temp]# pwd
/opt/temp
[root@kk01 temp]# cd csvout/
[root@kk01 csvout]# ll
total 4
-rw-r--r--. 1 root root 48 May 13 05:19 part-00000-70c37b99-e9ec-4005-abe6-90759ceed9ea-c000.csv
-rw-r--r--. 1 root root  0 May 13 05:19 _SUCCESS
[root@kk01 csvout]# cat part-00000-70c37b99-e9ec-4005-abe6-90759ceed9ea-c000.csv 
id,name,age
1,zhangsan,18
2,lisi,19
3,wnagwu,20
```

将DataFrame数据保存至CSV格式文件，演示代码如下：

```SCALA
mlRatingsDF
    // 降低分区数，此处设置为1，将所有数据保存到一个文件中
    .coalesce(1)
    .write
    // 设置保存模式，依据实际业务场景选择，此处为覆写
    .mode(SaveMode.Overwrite)
    .option("sep", ",")
    // TODO: 建议设置首行为列名
    .option("header", "true")
    .csv("datas/ml-csv-" + System.nanoTime())	
```



## 操作MySQL

在SparkCore中读取MySQL表的数据通过JdbcRDD来读取的，在SparkSQL模块中提供对应接口，提供三种方式读取数据

-   单分区模式

```scala
def jdbc(url: String, table: String, connectionProperties: Properties): DataFrame
```

-   多分区模式，可以设置列的名称，作为分区字段及列的值范围和分区数目

```scala
def jdbc(url: String,
         table: String, 
         cloumnName: String, // 列的名称，按照此列进行划分分区
         lowerBound: Long,
         upperBound: Long,
         numPartitions: Int,	// 表示读取数据分区数目（RDD分区数目）
         connectionProperties: Properties)
	: DataFrame
```

-   高度自由分区模式，通过设置条件语句设置分区数据及各个分区数据范围

```scala
def jdbc(url: String, 
         table: String, 
         // 谓词，就是条件语句，就是sql语句的where
         predicates: Array[String],
         connectionProperties: Properties)
: DataFrame
```

当加载读取RDBMS表的数据量不大时，可以直接使用单分区模式加载；当数据量很多时，考虑使用多分区及自由分区方式加载。 



-   Spark SQL可以**通过 JDBC 从关系型数据库中读取数据的方式创建 DataFrame**，通过对DataFrame 一系列的计算后，还可以将数据再写回关系型数据库中。
-   如果**使用spark-shell**操作，可在**启动shell时指定相关的数据库驱动路径或者将相关的数据库驱动放到spark的类路径下**。

```shell
bin/spark-shell --jars  mysql-connector-java-5.1.47.jar
```

导入pom依赖

```xml
<dependency>
	<groupId>mysql</groupId>
    <artifactId>mysql-connector-java</artifactId>
    <version>5.1.47</version>
</dependency>
```

### 1）加载数据

加载数据，即读取 MySQL数据库数据

```scala
[root@kk01 spark-local]# pwd
/opt/software/spark-local
[root@kk01 spark-local]# bin/spark-shell --jars /opt/temp/mysql-connector-java-5.1.47.jar 
// 我们连接的是宿主机（window平台）的mysql   
// 参数 useSSL=false 表示关闭安全连接
scala> var df = spark.read.format("jdbc").option("url","jdbc:mysql://192.168.43.2:3306/company?useSSL=false").option("driver","com.mysql.jdbc.Driver").option
("user","root").option("password","123456").option("dbtable","account").load()
var df: org.apache.spark.sql.DataFrame = [acc_no: int, acc_name: string ... 1 more field]

scala> df.show()
+------+--------+-------+
|acc_no|acc_name|balance|
+------+--------+-------+
|     1|    李三|    200|
|     2|    王五|   1800|
|     3|    王五|   1900|
|    10|    赵六|   3000|
|    20|    马七|   5000|
+------+--------+-------+
```

**如果报错：java.sql.SQLException: null,  message from server: "Host 'YOU' is not allowed to connect to this MySQL server"**

则说明宿主机的MySQL不允许远程访问，可以采用如下方式解决

```mysql
# 我们采取的是最简单暴力的方法，修改数据库权限
mysql> use mysql;
Database changed
mysql> select user,host from user;
+---------------+-----------+
| user          | host      |
+---------------+-----------+
| column_user   | localhost |
| mysql.session | localhost |
| mysql.sys     | localhost |
| root          | localhost |
| server_user   | localhost |
| st_01         | localhost |
+---------------+-----------+
6 rows in set (0.00 sec)

mysql> update user set host='%' where user='root';
Query OK, 1 row affected (0.01 sec)
Rows matched: 1  Changed: 1  Warnings: 0

# flush privileges是为了将权限更新操作刷新到内存中，而不用下次启动时生效。不用这句话的话重启mysql服务也行。
mysql> flush privileges;  
Query OK, 0 rows affected (0.01 sec)

mysql> select user,host from user;
+---------------+-----------+
| user          | host      |
+---------------+-----------+
| root          | %         |
| column_user   | localhost |
| mysql.session | localhost |
| mysql.sys     | localhost |
| server_user   | localhost |
| st_01         | localhost |
+---------------+-----------+
6 rows in set (0.00 sec)

mysql>
```

### 2）保存数据

保存数据，即 将DataFrame对象数据保存至MySQL数据库

```scala
// 将DataFrame对象数据保存至MySQL数据库，保存模式为：如果文件已经存在则忽略
scala> df.write.format("jdbc").option("url","jdbc:mysql://192.188.43.2:3306/company?useSSL=false").option("driver","com.mysql.jdbc.Driver").option("user","ro
ot").option("password","123456").option("dbtable","account").mode("ignore").save()
```

### 3）Java API

```java
/**
 * 通过JDBC从关系型数据库读取数据
 */
public class SparkSQL_JDBC {
    public static void main(String[] args) {
        SparkConf sparkConf = new SparkConf().setAppName("加载JSON文件~~~").setMaster("local[*]");
        JavaSparkContext jsc = new JavaSparkContext(sparkConf);
        jsc.setLogLevel("ERROR");  // 设置日志打印级别
        SparkSession spark = SparkSession
                .builder()
                .config(sparkConf)
                .getOrCreate();
        SQLContext sqlContext = new SQLContext(spark);

        // 读取MySQL数据
        Dataset<Row> df = spark.read()
                .format("jdbc")  
                .option("url","jdbc:mysql://localhost:3306/company")
                .option("driver","com.mysql.jdbc.Driver")
                .option("user","root")
                .option("password","123456")
                .option("dbtable","account")
                .load();

        df.cache();

        // 展示加载的数据
        df.show();

        // 保存 df 到MySQL
        df.write().format("jdbc")
                .option("url","jdbc:mysql://localhost:3306/company")
                .option("driver","com.mysql.jdbc.Driver")
                .option("user","root")
                .option("password","123456")
                .option("dbtable","account2")  // 保存为account2表
                .mode(SaveMode.Append)   // 如果文件已经存在则追加
                .save();
        
        spark.stop();
        jsc.stop();
    }
}
```

结果如下

读取的数据为

```diff
+------+--------+-------+
|acc_no|acc_name|balance|
+------+--------+-------+
|     1|    李三|    200|
|     2|    王五|   1800|
|     3|    王五|   1900|
|    10|    赵六|   3000|
|    20|    马七|   5000|
+------+--------+-------+
```

查看MySQL数据库

```mysql
mysql> use company;
Database changed
mysql> select * from account2;
+--------+----------+---------+
| acc_no | acc_name | balance |
+--------+----------+---------+
|      1 | 李三     |     200 |
|      2 | 王五     |    1800 |
|      3 | 王五     |    1900 |
|     10 | 赵六     |    3000 |
|     20 | 马七     |    5000 |
+--------+----------+---------+
5 rows in set (0.00 sec)

mysql>
```



## 操作Hive数据集

​	Apache Hive 是 Hadoop 上的 SQL 引擎，Spark SQL编译时可以包含 Hive 支持，也可以不包含。包含 Hive 支持的 Spark SQL 可以支持 Hive 表访问、UDF (用户自定义函数)以及 Hive 查询语言(HiveQL/HQL)等。需要强调的一点是，如果要在 Spark SQL 中包含Hive 的库，并不需要事先安装 Hive。

一般来说，最好还是在编译Spark SQL时引入Hive支持，这样就可以使用这些特性了。如果你下载的是二进制版本的 Spark，它应该已经在编译时添加了 Hive 支持。

若要把 Spark SQL 连接到一个部署好的 Hive 上，你必须把 hive-site.xml 复制到 Spark的配置文件目录中($SPARK_HOME/conf)。

即使没有部署好 Hive，Spark SQL 也可以运行。

 需要注意的是，如果你没有部署好Hive，Spark SQL 会在当前的工作目录中创建出自己的 Hive 元数据仓库，叫作 metastore_db。

此外，如果你尝试使用 HiveQL 中的 CREATE TABLE (并非 CREATE EXTERNAL TABLE)语句来创建表，这些表会被放在你默认的文件系统中的 /user/hive/warehouse 目录中(如果你的 classpath 中有配好的 hdfs-site.xml，默认的文件系统就是 HDFS，否则就是本地文件系统)。

**spark-shell默认是Hive支持的；代码中是默认不支持的，需要手动指定（加一个参数即可）。**

### 1）内嵌的Hive（不推荐使用）

如果使用 Spark 内嵌的 Hive, 则什么都不用做, 直接使用即可.

Hive 的**元数据存储在 derby 中**, 默认仓库地址:$SPARK_HOME/spark-warehouse

数据准备

在/opt/temp目录下有 ids.txt 文件，内容如下

```
1
2
3
```

操作内嵌Hive

```scala
scala> spark.sql("show tables").show()
+---------+---------+-----------+
|namespace|tableName|isTemporary|
+---------+---------+-----------+
+---------+---------+-----------+

scala> spark.sql("create table test1(id int)")

scala> spark.sql("show tables").show()
+---------+---------+-----------+
|namespace|tableName|isTemporary|
+---------+---------+-----------+
|  default|    test1|      false|
+---------+---------+-----------+
```

向表加载本地数据

```scala
scala> spark.sql("load data local inpath '/opt/temp/ids.txt' into table test1")
val res61: org.apache.spark.sql.DataFrame = []

scala> spark.sql("select * from test1").show()
+---+
| id|
+---+
|  1|
|  2|
|  3|
+---+
```

**在实际使用中, 几乎没有任何人会使用内置的 Hive**

### 2）外部的Hive

如果想连接外部已经部署好的Hive，需要通过以下几个步骤：

-   要把Spark SQL连接到一个已经部署好的Hive时， 就必须把hive-site.xml 拷贝到Spark的配置目录conf下（采用软链接的方式会更好一点）

    ```shell
    ln -s hive安装目录/conf/hive.site.xml spark安装目录/conf/hive.site.xml
    ```

-   **Hive采用MySQL数据库存放Hive元数据信息**，因此为了能够让Spark访问Hive，就需要把MySQL的驱动copy到jars/目录下

-   如果访问不到hdfs，则需要把core-site.xml和hdfs-site.xml拷贝到conf/目录下（采用软链接的方式会更好一点）

-   重启spark-shell

### 3）代码中使用Hive

用 Python 创建 HiveContext 并查询数据

```python
from pyspark.sql import HiveContext

hiveCtx = HiveContext(sc)
rows = hiveCtx.sql("select name, age from users")
firstRow = rows.first()
print(firstRow.name)
```

用 Scala 创建  HiveContext 并查询数据

```scala
import org.apache.spark.sql.hive.HiveContext

val hiveCtx = new HiveContext(sc)
val rows = hiveCtx.sql("select name, age from users")
val firstRow = rows.first()
println(firstRow.getString(0))  // 读取的是第一个字段
```

用 Java 创建  HiveContext 并查询数据

```java
import org.apache.spark.sql.hive.HiveContext;
import org.apache.spark.sql.Row;
import org.apache.spark.sql.SchemaRDD;

HiveContext hiveCtx = new HiveContext(jsc);
SchemaRDD rows = hiveCtx.sql("select name, age from users");
Row firstRow = rows.first();
System.out.printlnfirstRow.getAs("name")) 
```



