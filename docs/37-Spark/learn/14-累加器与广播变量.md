# 累加器与广播变量

## 共享变量（Shared Variables）

Spark计算框架为了能够进行高并发和高吞吐的数据处理，封装了三大数据结构，用于处理不同的应用场景。三大数据结构分别是：

-   RDD : 弹性分布式数据集
-   累加器：分布式共享**只写**变量
-   广播变量：分布式共享**只读**变量

在默认情况下，当Spark在集群的多个不同节点的多个任务上并行运行一个函数时，它会把函数中涉及到的每个变量，在每个任务上都生成一个副本。

但是，有时候需要在多个任务之间共享变量，或者在任务(Task)和任务控制节点(Driver Program)之间共享变量。为了满足这种需求，Spark提供了两种类型的变量： 

-   **1）广播变量Broadcast Variables** 
    -   广播变量用来把变量在所有节点的内存之间进行共享，在每个机器上缓存一个只读的变量，而不是为机器上的每个任务都生成一个副本（即==把某个变量发送到所以Executor，让其中所有的task共享这个变量的值，变量值不可变==） 
    -   广播变量类似与MapReduce中Map DistributedCache
-   **2）累加器Accumulators** 
    -   累加器支持在所有不同节点之间进行累加计算(比如计数或者求和)；
    -   累加器类似与MapReduce中计数器Counters

## 累加器 Accumulators 

前面介绍过：累计器是分布式共享**只写**变量（全局共享）**Executor和Executor之间不能读数据**

Spark提供的Accumulator，主要用于多个节点对一个变量进行共享性的操作。Accumulator只提供了累加的功能，即确提供了多个task对一个变量并行操作的功能。但是task只能对 Accumulator进行累加操作，不能读取Accumulator的值，只有Driver程序可以读取Accumulator的值。创建的Accumulator变量的值能够在Spark Web UI上看到，在创建时应该尽量为其命名。

### **为什么要使用累加器？**

通常在向Spark传递函数时，比如使用map算子或 filter算子传条件时，可以使用Driver端中定义的变量，但是集群中运行的每个任务都会得到这些变量的一份新的副本，更新这些副本的值也不会影响Driver端中对应的变量。

Spark的两个共享变量，累加器与广播变量，分别为结果聚合与广播这两种常见的通信模式突破了这一限制。

累加器：提供了将工作节点（Executor）中的值聚合到驱动器程序（Driver）中的简单语法。

累加器的常见用途：调试时对作业执行过程中的事件进行计数。



如果我们在Driver端定义一个变量，然后将该变量发送Executor端进行累加赋值操作，那么Driver端的变量值会发生改变吗？答案是不会，因为Executor端操作的是变量的副本，并不能影响Driver端的变量值。如何在这样的分布式系统中实现变量的共写呢？这就要用到累加器

### 实现原理

​	**累加器用来把Executor端变量信息聚合到Driver端**。在Driver程序中定义的变量，在Executor端的每个Task都会得到这个变量的一份新的副本，每个task更新这些副本的值后，传回Driver端进行merge。

### 累加器的用法

-   通过在Driver端调用 `sc.accumulator(initialValue) `方法，创建出存有初始值的累加器。返回值为 org.apache.spark.Accumulator[T] 对象，其中T是初始值initialValue的类型
-   Spark闭包里的执行执行器（Executor中）代码可以使用累加器的 += 方法（在Java中是 add方法）增加累加器的值。
-   Driver端可以调用累加器的 value 属性（在Java中使用 value() 或 setValue()方法）来访问累加器的值。

注意：

​	Executor节点上的任务不能访问累加器的值。从这个角度来看，累加器是一个只写变量。在这种模式下，累加器的实现可以更加高效，不需要对每次更新操作进行复制的通信。


### 基础编程

#### 系统累加器

Spark内置了三种类型的Accumulator，分别是LongAccumulator用来累加整数型，DoubleAccumulator用来累加浮点型，CollectionAccumulator用来累加集合元素

```scala
package org.apache.spark.util;  

def longAccumulator(name: String): LongAccumulator 
def doubleAccumulator(name: String): DoubleAccumulator
def collectionAccumulator[T](name: String): CollectionAccumulator[T]
```

如果是Java中使用累加器，需要先使用 JavaSparkContext 对象调用 sc()方法获取SparkContext对象，再调用上述相应方法获得spark提供的累加器

```java
// 使用JavaSparkContext对象调用该方法获取 SparkContext对象，再去使用累加器
public SparkContext sc() {
        return this.sc;
}

// private final SparkContext sc;
```



**1）使用RDD的方式**

```java
public static void main(String[] args) {
    SparkConf sparkConf = new SparkConf().setMaster("local[*]").setAppName("rdd实现方式");
    JavaSparkContext sc = new JavaSparkContext(sparkConf);
    JavaRDD<Integer> rdd = sc.parallelize(Arrays.asList(1, 2, 3, 4),2);

    Integer reduce = rdd.reduce((x, y) -> x + y);
    System.out.println(reduce);  // 10
}
```

使用reduce算子可以完成我们的需求，但是该算子存在shuffle操作

​	在很多场合我们都可以巧妙利用**累加器替代转换算子实现一些功能**，**避免**转换算子带来的**shuffle操作，从而提升程序性能**

**2）使用累加器的方式**

```java
public static void main(String[] args) {
    SparkConf sparkConf = new SparkConf().setMaster("local[*]").setAppName("ACC实现方式");
    JavaSparkContext sc = new JavaSparkContext(sparkConf);
    JavaRDD<Integer> rdd = sc.parallelize(Arrays.asList(1, 2, 3, 4),2);

    // spark默认提供了简单数据聚合的累加器
    // 使用累加器，能够将executor端执行结果回收到driver端
    LongAccumulator sumAcc = sc.sc().longAccumulator();

    // 使用foreach算子完成功能，避免使用reduce算子，从而避免shuffle操作
    rdd.foreach(
        // 使用累加器
        num -> sumAcc.add(num)
        // Executor端的任务不能读取累加器的值，因为累加器是一个分布式共享只写变量。
    );
    // 获取累加器的值
    System.out.println(sumAcc.value());
    sc.stop();
}

// 下面是使用累加器可能会遇到的一些问题
// 少加：转换算子中调用累加器，如果没有行动算子的话，那么不会执行
// 多加：转换算子中调用累加器，如果没有行动算子的话，那么不会执行
//  所以，一般情况下，累加器会放置在行动算子进行操作
```

**小结**

​	累加器要放在行动算子中，因为转换算子执行的次数取决于job的数量，如果一个spark应用有多个行动算子，那么转换算子中的累加器可能会发生不止一次更新，导致结果错误。所以，如果想要一个无论在失败还是重复计算时都绝对可靠的累加器，我们必须把它放在foreach()这样的行动算子中。

注意：

​	累加器在 Driver 端定义赋初始值，**累加器只能在 Driver端读取，在 Executor端更新**

#### 自定义累加器

当内置的Accumulator无法满足要求时，可以继承AccumulatorV2实现自定义的累加器。实现自定义累加器的步骤： 

-   继承AccumulatorV2，实现相关方法； 
-   创建自定义Accumulator的实例，然后在SparkContext上注册它

```java
/**
 * 自定义累加器
 */
public class SparkACC_demo3 {
    public static void main(String[] args) {
        //环境准备
        SparkConf sparkConf = new SparkConf().setAppName("词频统计，演示自定义累加器").setMaster("local[*]");
        JavaSparkContext jsc = new JavaSparkContext(sparkConf);

        List<String> strings = Arrays.asList("hello java", "hello world", "hello spark");
        JavaRDD<String> javaRDD = jsc.parallelize(strings);
        // 创建累加器对象
        MyAccumulator myAccumulator = new MyAccumulator();
        // 向Spark进行注册累加器
        jsc.sc().register(myAccumulator, "wordCountAcc");

        javaRDD.foreach(word -> {
            //使用累加器(进行词频统计)
            myAccumulator.add(word);
        });

        // 获取累加器结果
        Map<String, Integer> value = myAccumulator.value();
        value.forEach((k,v) -> {
            System.out.println(k + ":" + v);
        });
    }
}

/**
 * 自定义累加器类
 * 1.继承AccumulatorV2
 * 2.定义泛型AccumulatorV2<IN, OUT>
 * IN 累加器输入的数据类型
 * OUT 累加器返回的数据类型
 * 3.重写方法
 */
class MyAccumulator extends AccumulatorV2<String, Map<String, Integer>> {

    //定义输出类型变量
    private Map<String, Integer> output = new HashMap<>();

    // 判断是否为初始状态
    @Override
    public boolean isZero() {
        return this.output.isEmpty();
    }

    // 复制累加器
    @Override
    public AccumulatorV2<String, Map<String, Integer>> copy() {
        MyAccumulator myAccumulator = new MyAccumulator();
        //将此累加器中的数据赋值给新创建的累加器
        myAccumulator.output = this.output;
        return myAccumulator;
    }

    // 重置累加器
    @Override
    public void reset() {
        this.output.clear();
    }

    // 累加器添加元素
    @Override
    public void add(String v) {
        String[] split = v.split(" ");
        for (String s : split) {
            // 存在则加一，不存在则为一
            int value = this.output.getOrDefault(s, 0) + 1;
            this.output.put(s, value);
        }
    }

    // 合并累加器元素（Driver端合并多个累加器）
    @Override
    public void merge(AccumulatorV2<String, Map<String, Integer>> other) {
        other.value().forEach((K, V) -> {
            if (this.output.containsKey(K)) {
                Integer i1 = this.output.get(K);
                Integer i2 = other.value().get(K);
                this.output.put(K, i1 + i2);
            } else {
                this.output.put(K, V);
            }
        });
    }

    // 输出（累加器结果）
    @Override
    public Map<String, Integer> value() {
        return this.out
}
```

结果

```
world:1
java:1
spark:1
hello:3
```



## 广播变量 Broadcast Variables 

前面介绍过：广播变量是分布式共享**只读**变量

​	广播变量用来高效分发较大的对象。向所有工作节点发送一个较大的只读值，以供一个或多个Spark操作使用。比如，如果你的应用需要向所有节点发送一个较大的只读查询表，广播变量用起来都很顺手。在多个并行操作中使用同一个变量，但是 Spark会为每个任务分别发送。

### **spark官方介绍**

​	广播变量是Spark中另一种共享变量，允许程序将一个只读的变量发送到Executor，一个Executor只需要在第一个Task启动时，获得一份Broadcast数据，之后的Task都从本节点的BlockManager中获取相关数据。

​	广播变量是通过调用SparkContext.broadcast(v)从变量v创建的。广播变量是v的包装器，它的值可以通过调用value方法来访问。

### **广播变量演示**

```java
/**
* 不使用广播变量，每个task都会获得一个变量副本（数据量大，并行度高，可能导致性能较差）
* 广播变量使用方式
* 广播变量将变量的复制次数，从task的数量减少到进程executor个数，达到资源公用的目的
*/
public static void main(String[] args) {
        SparkConf sparkConf = new SparkConf().setMaster("local").setAppName("broadcast");
        JavaSparkContext jsc = new JavaSparkContext(sparkConf);
        jsc.setLogLevel("WARN");

        JavaRDD<String> rdd = jsc.parallelize(Arrays.asList("hello", "hello", "world", "java", "spark", "spark"), 3);

        // 定义广播变量
        List<String> list = Arrays.asList("hello", "spark");
        Broadcast<List<String>> broadcast = jsc.broadcast(list);

        // 演示使用广播变量用来过滤
        JavaRDD<String> filter = rdd.filter(new Function<String, Boolean>() {
            @Override
            public Boolean call(String word) throws Exception {
                List<String> value = broadcast.value();
                return value.contains(word);
            }
        });
        filter.foreach(new VoidFunction<String>() {
            @Override
            public void call(String s) throws Exception {
                System.out.println(s);
            }
        });
    }
```

结果

```
hello
hello
spark
spark
```

### **小结**

-   **广播变量只能在 Driver 端定义，不能在 Executor端定义**

-   在 Driver 端可以修改广播变量的值，**在Executor 端无法修改广播变量的值**

-   闭包数据，都是以Task为单位发送的，每个任务中包含闭包数据

-   这样可能会导致，一个Executor中含有大量重复的数据，并且占用大量的内存

-   Executor其实就是一个JVM，所以在启动时，会自动分配内存

    -   完成可以将任务中的闭包数据放置在Executor的内存中，达到共享的目的

-   Spark的广播变量就可以将闭包的数据保存到Executor的内存中

-   Spark中的广播变量不能够更改：分布式共享**只读**变量，如下源码所示

    ```scala
    /**
    * 广播一个只读变量到集群，返回一个 [[org.apache.spark.broadcast.Broadcast]] 对象用于在分布式函* 数中读取它，该变量将只发送到每个集群一次
    */
    def broadcast[T](value: T): Broadcast[T] = sc.broadcast(value)(fokeClassTag)
    ```



## 演示

完成词频统计，对非单词字符（! ? 等）使用广播变量发送至所有Executor，完成进行过滤，并使用累加器进行统计

```scala
import org.apache.spark.broadcast.Broadcast
import org.apache.spark.util.LongAccumulator
import org.apache.spark.{SparkConf, SparkContext}
import org.apache.spark.rdd.RDD


/**
 * 基于Spark框架使用Scala语言编程实现词频统计WordCount程序，将符号数据过滤，并统计出现的次数
 */
object SparkSharedVariableTest {
  def main(args: Array[String]): Unit = {
    // todo 创建SparkContext对象，需要传递SparkConf对象，设置应用配置信息
    val conf = new SparkConf()
      .setAppName("词频统计")
      .setMaster("local[2]")
    val sc = new SparkContext(conf)

    // 定义脏数据，只有包含在其中就过滤
    val li = List("!", "?", ",", "$", "#", "&")
    // todo 通过广播变量：将li发送到所有Executor内存中，便于每个Executor中的多个Task使用
    val listBroadcast: Broadcast[List[String]] = sc.broadcast(li)
    // todo 定义累加器：记录脏数据的数量
    val acc: LongAccumulator = sc.longAccumulator("number_acc")
    
    // todo 读取数据，封装数据到RDD
    val list = List("hello world ! mysql spark ? hello \n hello world ! mysql spark ? hello")
    val inputRDD: RDD[String] = sc.parallelize(list, 2)

    val resultRDD = inputRDD
      // 过滤空数据
      .filter(line => null != line && line.trim.length != 0)
      // 对每行数据分割单词
      .flatMap(line => line.trim.split("\\s+"))
      // todo 过滤非单词数据，并使用累加器记录
      .filter(word => {
        //      !li.contains(word)  // 直接使用Driver端传过来的List，每个Task一份，性能底下
        // todo 获取广播变量的值
        val listValue: List[String] = listBroadcast.value // 使用广播变量，每个Executor一份List，性能较高
        val isFlag: Boolean = listValue.contains(word)
        if (isFlag) {
          acc.add(1) // todo 累加器 +1
        }
        // 返回值
        !isFlag
      })

      .map(word => (word, 1))
      .reduceByKey((tmp, item) => tmp + item)
    // 保存数据，将最终RDD结果数据保存至外部存储系统
    resultRDD.foreach(tuple => println(tuple))

    // todo 获取累加器的值
    // 注意：获取累加器的值，需要action算子的触发
    println(s"number_acc = ${acc}")

    // 睡眠线程，方便在Spark Web UI上看到累加器变量
    Thread.sleep(1000000)

    // 应用程序结束，关闭资源
    sc.stop()

  }
}
```

